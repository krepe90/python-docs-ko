# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2001-2017, Python Software Foundation
# This file is distributed under the same license as the Python package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2017.
#
msgid ""
msgstr ""
"Project-Id-Version: Python 3.6\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-04-17 23:44+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: Dong-gweon Oh <flowdas@gmail.com>\n"
"Language-Team: Korean (https://python.flowdas.com)\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../library/urllib.robotparser.rst:2
#, fuzzy
msgid ":mod:`!urllib.robotparser` ---  Parser for robots.txt"
msgstr ":mod:`urllib.robotparser` --- robots.txt 구문 분석기"

#: ../../library/urllib.robotparser.rst:10
msgid "**Source code:** :source:`Lib/urllib/robotparser.py`"
msgstr "**소스 코드:** :source:`Lib/urllib/robotparser.py`"

#: ../../library/urllib.robotparser.rst:20
#, fuzzy
msgid ""
"This module provides a single class, :class:`RobotFileParser`, which "
"answers questions about whether or not a particular user agent can fetch "
"a URL on the web site that published the :file:`robots.txt` file.  For "
"more details on the structure of :file:`robots.txt` files, see "
"http://www.robotstxt.org/orig.html."
msgstr ""
"이 모듈은 클래스 하나 :class:`RobotFileParser`\\를 제공하는데, 특정 사용자 에이전트가 "
":file:`robots.txt` 파일을 게시한 웹 사이트에서 URL을 가져올 수 있는지에 대한 질문에 대답합니다. "
":file:`robots.txt` 파일의 구조에 대한 자세한 내용은 http://www.robotstxt.org/orig.html "
"을 참조하십시오."

#: ../../library/urllib.robotparser.rst:28
msgid ""
"This class provides methods to read, parse and answer questions about the"
" :file:`robots.txt` file at *url*."
msgstr ""
"이 클래스는 *url*\\에 있는 :file:`robots.txt` 파일을 읽고, 구문 분석하고, 그에 대한 질문에 대답하는 "
"메서드를 제공합니다."

#: ../../library/urllib.robotparser.rst:33
msgid "Sets the URL referring to a :file:`robots.txt` file."
msgstr ":file:`robots.txt` 파일을 가리키는 URL을 설정합니다."

#: ../../library/urllib.robotparser.rst:37
msgid "Reads the :file:`robots.txt` URL and feeds it to the parser."
msgstr ":file:`robots.txt` URL을 읽어서 구문 분석기에 넘깁니다."

#: ../../library/urllib.robotparser.rst:41
msgid "Parses the lines argument."
msgstr "lines 인자를 구문 분석합니다."

#: ../../library/urllib.robotparser.rst:45
msgid ""
"Returns ``True`` if the *useragent* is allowed to fetch the *url* "
"according to the rules contained in the parsed :file:`robots.txt` file."
msgstr ""
"구문 분석된 :file:`robots.txt` 파일에 포함된 규칙에 따라, *useragent*\\가 *url*\\를 가져올 수 "
"있으면 ``True``\\를 반환합니다."

#: ../../library/urllib.robotparser.rst:51
msgid ""
"Returns the time the ``robots.txt`` file was last fetched.  This is "
"useful for long-running web spiders that need to check for new "
"``robots.txt`` files periodically."
msgstr ""
"``robots.txt`` 파일을 마지막으로 가져온 시간을 반환합니다. 이것은 새 ``robots.txt`` 파일을 주기적으로 "
"확인해야 하는 장기 실행 웹 스파이더에 유용합니다."

#: ../../library/urllib.robotparser.rst:57
msgid ""
"Sets the time the ``robots.txt`` file was last fetched to the current "
"time."
msgstr "``robots.txt`` 파일을 마지막으로 가져온 시간을 현재 시각으로 설정합니다."

#: ../../library/urllib.robotparser.rst:62
msgid ""
"Returns the value of the ``Crawl-delay`` parameter from ``robots.txt`` "
"for the *useragent* in question.  If there is no such parameter or it "
"doesn't apply to the *useragent* specified or the ``robots.txt`` entry "
"for this parameter has invalid syntax, return ``None``."
msgstr ""
"``robots.txt``\\에서 해당 *useragent*\\에 대한 ``Crawl-delay`` 파라미터의 값을 반환합니다. "
"해당 파라미터가 없거나, 지정된 *useragent*\\에 적용되지 않거나, 이 파라미터에 대한 ``robots.txt`` 항목이 "
"잘못된 구문이면 ``None``\\을 반환합니다."

#: ../../library/urllib.robotparser.rst:71
msgid ""
"Returns the contents of the ``Request-rate`` parameter from "
"``robots.txt`` as a :term:`named tuple` ``RequestRate(requests, "
"seconds)``. If there is no such parameter or it doesn't apply to the "
"*useragent* specified or the ``robots.txt`` entry for this parameter has "
"invalid syntax, return ``None``."
msgstr ""
"``robots.txt``\\에서 ``Request-rate`` 파라미터의 내용을 :term:`네임드 튜플 <named "
"tuple>` ``RequestRate(requests, seconds)``\\로 반환합니다. 해당 파라미터가 없거나, 지정된 "
"*useragent*\\에 적용되지 않거나, 이 파라미터에 대한 ``robots.txt`` 항목이 잘못된 구문이면 "
"``None``\\을 반환합니다."

#: ../../library/urllib.robotparser.rst:81
msgid ""
"Returns the contents of the ``Sitemap`` parameter from ``robots.txt`` in "
"the form of a :func:`list`. If there is no such parameter or the "
"``robots.txt`` entry for this parameter has invalid syntax, return "
"``None``."
msgstr ""
"``robots.txt``\\에서 ``Sitemap`` 매개 변수의 내용을 :func:`list` 형식으로 반환합니다. 해당 매개 "
"변수가 없거나 ``robots.txt``\\의 이 매개 변수 항목이 잘못된 문법이면 ``None``\\을 반환합니다."

#: ../../library/urllib.robotparser.rst:89
msgid ""
"The following example demonstrates basic use of the "
":class:`RobotFileParser` class::"
msgstr "다음 예제는 :class:`RobotFileParser` 클래스의 기본 사용을 보여줍니다::"

#: ../../library/urllib.robotparser.rst:92
msgid ""
">>> import urllib.robotparser\n"
">>> rp = urllib.robotparser.RobotFileParser()\n"
">>> rp.set_url(\"http://www.musi-cal.com/robots.txt\")\n"
">>> rp.read()\n"
">>> rrate = rp.request_rate(\"*\")\n"
">>> rrate.requests\n"
"3\n"
">>> rrate.seconds\n"
"20\n"
">>> rp.crawl_delay(\"*\")\n"
"6\n"
">>> rp.can_fetch(\"*\", \"http://www.musi-cal.com/cgi-"
"bin/search?city=San+Francisco\")\n"
"False\n"
">>> rp.can_fetch(\"*\", \"http://www.musi-cal.com/\")\n"
"True"
msgstr ""

#: ../../library/urllib.robotparser.rst:12
msgid "WWW"
msgstr ""

#: ../../library/urllib.robotparser.rst:12
msgid "World Wide Web"
msgstr ""

#: ../../library/urllib.robotparser.rst:12
msgid "URL"
msgstr ""

#: ../../library/urllib.robotparser.rst:12
msgid "robots.txt"
msgstr ""

